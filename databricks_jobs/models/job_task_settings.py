# coding: utf-8

"""
    Jobs API 2.1

    The Jobs API allows you to create, edit, and delete jobs. You should never hard code secrets or store them in plain text. Use the [Secrets API](https://docs.microsoft.com/azure/databricks/dev-tools/api/latest/secrets) to manage secrets in the [Databricks CLI](https://docs.microsoft.com/azure/databricks/dev-tools/cli/index). Use the [Secrets utility](https://docs.microsoft.com/azure/databricks/dev-tools/databricks-utils#dbutils-secrets) to reference secrets in notebooks and jobs.  # noqa: E501

    The version of the OpenAPI document: 2.1
    Generated by: https://openapi-generator.tech
"""


from __future__ import annotations

import json
import pprint
import re  # noqa: F401
from inspect import getfullargspec
from typing import List, Optional

from pydantic import (
    BaseModel,
    Field,
    StrictBool,
    StrictInt,
    StrictStr,
    constr,
    validator,
)

from databricks_jobs.models.dbt_task import DbtTask
from databricks_jobs.models.job_email_notifications import JobEmailNotifications
from databricks_jobs.models.library import Library
from databricks_jobs.models.new_task_cluster import NewTaskCluster
from databricks_jobs.models.notebook_task import NotebookTask
from databricks_jobs.models.pipeline_task import PipelineTask
from databricks_jobs.models.python_wheel_task import PythonWheelTask
from databricks_jobs.models.spark_jar_task import SparkJarTask
from databricks_jobs.models.spark_python_task import SparkPythonTask
from databricks_jobs.models.sql_task import SqlTask
from databricks_jobs.models.task_dependencies_inner import TaskDependenciesInner
from databricks_jobs.models.task_spark_submit_task import TaskSparkSubmitTask


class JobTaskSettings(BaseModel):
    """NOTE: This class is auto generated by OpenAPI Generator.
    Ref: https://openapi-generator.tech

    Do not edit the class manually.
    """

    task_key: constr(strict=True, max_length=100, min_length=1) = Field(
        ...,
        description="A unique name for the task. This field is used to refer to this task from other tasks. This field is required and must be unique within its parent job. On Update or Reset, this field is used to reference the tasks to be updated or reset. The maximum length is 100 characters.",
    )
    description: Optional[constr(strict=True, max_length=4096)] = Field(
        None,
        description="An optional description for this task. The maximum length is 4096 bytes.",
    )
    depends_on: Optional[List[TaskDependenciesInner]] = Field(
        None,
        description="An optional array of objects specifying the dependency graph of the task. All tasks specified in this field must complete successfully before executing this task. The key is `task_key`, and the value is the name assigned to the dependent task. This field is required when a job consists of more than one task.",
    )
    existing_cluster_id: Optional[StrictStr] = Field(
        None,
        description="If existing_cluster_id, the ID of an existing cluster that is used for all runs of this task. When running tasks on an existing cluster, you may need to manually restart the cluster if it stops responding. We suggest running jobs on new clusters for greater reliability.",
    )
    new_cluster: Optional[NewTaskCluster] = None
    job_cluster_key: Optional[
        constr(strict=True, max_length=100, min_length=1)
    ] = Field(
        None,
        description="If job_cluster_key, this task is executed reusing the cluster specified in `job.settings.job_clusters`.",
    )
    notebook_task: Optional[NotebookTask] = None
    spark_jar_task: Optional[SparkJarTask] = None
    spark_python_task: Optional[SparkPythonTask] = None
    spark_submit_task: Optional[TaskSparkSubmitTask] = None
    pipeline_task: Optional[PipelineTask] = None
    python_wheel_task: Optional[PythonWheelTask] = None
    sql_task: Optional[SqlTask] = None
    dbt_task: Optional[DbtTask] = None
    libraries: Optional[List[Library]] = Field(
        None,
        description="An optional list of libraries to be installed on the cluster that executes the task. The default value is an empty list.",
    )
    email_notifications: Optional[JobEmailNotifications] = None
    timeout_seconds: Optional[StrictInt] = Field(
        None,
        description="An optional timeout applied to each run of this job task. The default behavior is to have no timeout.",
    )
    max_retries: Optional[StrictInt] = Field(
        None,
        description="An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with the `FAILED` result_state or `INTERNAL_ERROR` `life_cycle_state`. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.",
    )
    min_retry_interval_millis: Optional[StrictInt] = Field(
        None,
        description="An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.",
    )
    retry_on_timeout: Optional[StrictBool] = Field(
        None,
        description="An optional policy to specify whether to retry a task when it times out. The default behavior is to not retry on timeout.",
    )
    __properties = [
        "task_key",
        "description",
        "depends_on",
        "existing_cluster_id",
        "new_cluster",
        "job_cluster_key",
        "notebook_task",
        "spark_jar_task",
        "spark_python_task",
        "spark_submit_task",
        "pipeline_task",
        "python_wheel_task",
        "sql_task",
        "dbt_task",
        "libraries",
        "email_notifications",
        "timeout_seconds",
        "max_retries",
        "min_retry_interval_millis",
        "retry_on_timeout",
    ]

    @validator("task_key")
    def task_key_validate_regular_expression(cls, v):
        if not re.match(r"^[\w\-]+$", v):
            raise ValueError(r"must validate the regular expression /^[\w\-]+$/")
        return v

    @validator("job_cluster_key")
    def job_cluster_key_validate_regular_expression(cls, v):
        if not re.match(r"^[\w\-]+$", v):
            raise ValueError(r"must validate the regular expression /^[\w\-]+$/")
        return v

    class Config:
        allow_population_by_field_name = True
        validate_assignment = True

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.dict(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> JobTaskSettings:
        """Create an instance of JobTaskSettings from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self):
        """Returns the dictionary representation of the model using alias"""
        _dict = self.dict(by_alias=True, exclude={}, exclude_none=True)
        # override the default output from pydantic by calling `to_dict()` of each item in depends_on (list)
        _items = []
        if self.depends_on:
            for _item in self.depends_on:
                if _item:
                    _items.append(_item.to_dict())
            _dict["depends_on"] = _items
        # override the default output from pydantic by calling `to_dict()` of new_cluster
        if self.new_cluster:
            _dict["new_cluster"] = self.new_cluster.to_dict()
        # override the default output from pydantic by calling `to_dict()` of notebook_task
        if self.notebook_task:
            _dict["notebook_task"] = self.notebook_task.to_dict()
        # override the default output from pydantic by calling `to_dict()` of spark_jar_task
        if self.spark_jar_task:
            _dict["spark_jar_task"] = self.spark_jar_task.to_dict()
        # override the default output from pydantic by calling `to_dict()` of spark_python_task
        if self.spark_python_task:
            _dict["spark_python_task"] = self.spark_python_task.to_dict()
        # override the default output from pydantic by calling `to_dict()` of spark_submit_task
        if self.spark_submit_task:
            _dict["spark_submit_task"] = self.spark_submit_task.to_dict()
        # override the default output from pydantic by calling `to_dict()` of pipeline_task
        if self.pipeline_task:
            _dict["pipeline_task"] = self.pipeline_task.to_dict()
        # override the default output from pydantic by calling `to_dict()` of python_wheel_task
        if self.python_wheel_task:
            _dict["python_wheel_task"] = self.python_wheel_task.to_dict()
        # override the default output from pydantic by calling `to_dict()` of sql_task
        if self.sql_task:
            _dict["sql_task"] = self.sql_task.to_dict()
        # override the default output from pydantic by calling `to_dict()` of dbt_task
        if self.dbt_task:
            _dict["dbt_task"] = self.dbt_task.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in libraries (list)
        _items = []
        if self.libraries:
            for _item in self.libraries:
                if _item:
                    _items.append(_item.to_dict())
            _dict["libraries"] = _items
        # override the default output from pydantic by calling `to_dict()` of email_notifications
        if self.email_notifications:
            _dict["email_notifications"] = self.email_notifications.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: dict) -> JobTaskSettings:
        """Create an instance of JobTaskSettings from a dict"""
        if obj is None:
            return None

        if type(obj) is not dict:
            return JobTaskSettings.parse_obj(obj)

        _obj = JobTaskSettings.parse_obj(
            {
                "task_key": obj.get("task_key"),
                "description": obj.get("description"),
                "depends_on": [
                    TaskDependenciesInner.from_dict(_item)
                    for _item in obj.get("depends_on")
                ]
                if obj.get("depends_on") is not None
                else None,
                "existing_cluster_id": obj.get("existing_cluster_id"),
                "new_cluster": NewTaskCluster.from_dict(obj.get("new_cluster"))
                if obj.get("new_cluster") is not None
                else None,
                "job_cluster_key": obj.get("job_cluster_key"),
                "notebook_task": NotebookTask.from_dict(obj.get("notebook_task"))
                if obj.get("notebook_task") is not None
                else None,
                "spark_jar_task": SparkJarTask.from_dict(obj.get("spark_jar_task"))
                if obj.get("spark_jar_task") is not None
                else None,
                "spark_python_task": SparkPythonTask.from_dict(
                    obj.get("spark_python_task")
                )
                if obj.get("spark_python_task") is not None
                else None,
                "spark_submit_task": TaskSparkSubmitTask.from_dict(
                    obj.get("spark_submit_task")
                )
                if obj.get("spark_submit_task") is not None
                else None,
                "pipeline_task": PipelineTask.from_dict(obj.get("pipeline_task"))
                if obj.get("pipeline_task") is not None
                else None,
                "python_wheel_task": PythonWheelTask.from_dict(
                    obj.get("python_wheel_task")
                )
                if obj.get("python_wheel_task") is not None
                else None,
                "sql_task": SqlTask.from_dict(obj.get("sql_task"))
                if obj.get("sql_task") is not None
                else None,
                "dbt_task": DbtTask.from_dict(obj.get("dbt_task"))
                if obj.get("dbt_task") is not None
                else None,
                "libraries": [
                    Library.from_dict(_item) for _item in obj.get("libraries")
                ]
                if obj.get("libraries") is not None
                else None,
                "email_notifications": JobEmailNotifications.from_dict(
                    obj.get("email_notifications")
                )
                if obj.get("email_notifications") is not None
                else None,
                "timeout_seconds": obj.get("timeout_seconds"),
                "max_retries": obj.get("max_retries"),
                "min_retry_interval_millis": obj.get("min_retry_interval_millis"),
                "retry_on_timeout": obj.get("retry_on_timeout"),
            }
        )
        return _obj
