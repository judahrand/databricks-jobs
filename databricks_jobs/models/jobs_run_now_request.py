# coding: utf-8

"""
    Jobs API 2.1

    The Jobs API allows you to create, edit, and delete jobs. You should never hard code secrets or store them in plain text. Use the [Secrets API](https://docs.microsoft.com/azure/databricks/dev-tools/api/latest/secrets) to manage secrets in the [Databricks CLI](https://docs.microsoft.com/azure/databricks/dev-tools/cli/index). Use the [Secrets utility](https://docs.microsoft.com/azure/databricks/dev-tools/databricks-utils#dbutils-secrets) to reference secrets in notebooks and jobs.  # noqa: E501

    The version of the OpenAPI document: 2.1
    Generated by: https://openapi-generator.tech
"""


from __future__ import annotations

import json
import pprint
import re  # noqa: F401
from inspect import getfullargspec
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, StrictInt, StrictStr

from databricks_jobs.models.run_parameters_pipeline_params import (
    RunParametersPipelineParams,
)


class JobsRunNowRequest(BaseModel):
    """NOTE: This class is auto generated by OpenAPI Generator.
    Ref: https://openapi-generator.tech

    Do not edit the class manually.
    """

    job_id: Optional[StrictInt] = Field(
        None, description="The ID of the job to be executed"
    )
    idempotency_token: Optional[StrictStr] = Field(
        None,
        description="An optional token to guarantee the idempotency of job run requests. If a run with the provided token already exists, the request does not create a new run but returns the ID of the existing run instead. If a run with the provided token is deleted, an error is returned.  If you specify the idempotency token, upon failure you can retry until the request succeeds. Databricks guarantees that exactly one run is launched with that idempotency token.  This token must have at most 64 characters.  For more information, see [How to ensure idempotency for jobs](https://docs.microsoft.com/azure/databricks/kb/jobs/jobs-idempotency).",
    )
    jar_params: Optional[List[StrictStr]] = Field(
        None,
        description='A list of parameters for jobs with Spark JAR tasks, for example `"jar_params": ["john doe", "35"]`. The parameters are used to invoke the main function of the main class specified in the Spark JAR task. If not specified upon `run-now`, it defaults to an empty list. jar_params cannot be specified in conjunction with notebook_params. The JSON representation of this field (for example `{"jar_params":["john doe","35"]}`) cannot exceed 10,000 bytes.  Use [Task parameter variables](https://docs.microsoft.com/azure/databricks/jobs#parameter-variables) to set parameters containing information about job runs.',
    )
    notebook_params: Optional[Dict[str, Any]] = Field(
        None,
        description='A map from keys to values for jobs with notebook task, for example `"notebook_params": {"name": "john doe", "age": "35"}`. The map is passed to the notebook and is accessible through the [dbutils.widgets.get](https://docs.microsoft.com/azure/databricks/dev-tools/databricks-utils#dbutils-widgets) function.  If not specified upon `run-now`, the triggered run uses the jobâ€™s base parameters.  notebook_params cannot be specified in conjunction with jar_params.  Use [Task parameter variables](https://docs.microsoft.com/azure/databricks/jobs#parameter-variables) to set parameters containing information about job runs.  The JSON representation of this field (for example `{"notebook_params":{"name":"john doe","age":"35"}}`) cannot exceed 10,000 bytes.',
    )
    python_params: Optional[List[StrictStr]] = Field(
        None,
        description='A list of parameters for jobs with Python tasks, for example `"python_params": ["john doe", "35"]`. The parameters are passed to Python file as command-line parameters. If specified upon `run-now`, it would overwrite the parameters specified in job setting. The JSON representation of this field (for example `{"python_params":["john doe","35"]}`) cannot exceed 10,000 bytes.  Use [Task parameter variables](https://docs.microsoft.com/azure/databricks/jobs#parameter-variables) to set parameters containing information about job runs.  Important  These parameters accept only Latin characters (ASCII character set). Using non-ASCII characters returns an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.',
    )
    spark_submit_params: Optional[List[StrictStr]] = Field(
        None,
        description='A list of parameters for jobs with spark submit task, for example `"spark_submit_params": ["--class", "org.apache.spark.examples.SparkPi"]`. The parameters are passed to spark-submit script as command-line parameters. If specified upon `run-now`, it would overwrite the parameters specified in job setting. The JSON representation of this field (for example `{"python_params":["john doe","35"]}`) cannot exceed 10,000 bytes.  Use [Task parameter variables](https://docs.microsoft.com/azure/databricks/jobs#parameter-variables) to set parameters containing information about job runs.  Important  These parameters accept only Latin characters (ASCII character set). Using non-ASCII characters returns an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.',
    )
    python_named_params: Optional[Dict[str, Any]] = Field(
        None,
        description='A map from keys to values for jobs with Python wheel task, for example `"python_named_params": {"name": "task", "data": "dbfs:/path/to/data.json"}`.',
    )
    pipeline_params: Optional[RunParametersPipelineParams] = None
    sql_params: Optional[Dict[str, Any]] = Field(
        None,
        description='A map from keys to values for SQL tasks, for example `"sql_params": {"name": "john doe", "age": "35"}`. The SQL alert task does not support custom parameters.',
    )
    dbt_commands: Optional[List[StrictStr]] = Field(
        None,
        description='An array of commands to execute for jobs with the dbt task, for example `"dbt_commands": ["dbt deps", "dbt seed", "dbt run"]`',
    )
    __properties = [
        "job_id",
        "idempotency_token",
        "jar_params",
        "notebook_params",
        "python_params",
        "spark_submit_params",
        "python_named_params",
        "pipeline_params",
        "sql_params",
        "dbt_commands",
    ]

    class Config:
        allow_population_by_field_name = True
        validate_assignment = True

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.dict(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> JobsRunNowRequest:
        """Create an instance of JobsRunNowRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self):
        """Returns the dictionary representation of the model using alias"""
        _dict = self.dict(by_alias=True, exclude={}, exclude_none=True)
        # override the default output from pydantic by calling `to_dict()` of pipeline_params
        if self.pipeline_params:
            _dict["pipeline_params"] = self.pipeline_params.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: dict) -> JobsRunNowRequest:
        """Create an instance of JobsRunNowRequest from a dict"""
        if obj is None:
            return None

        if type(obj) is not dict:
            return JobsRunNowRequest.parse_obj(obj)

        _obj = JobsRunNowRequest.parse_obj(
            {
                "job_id": obj.get("job_id"),
                "idempotency_token": obj.get("idempotency_token"),
                "jar_params": obj.get("jar_params"),
                "notebook_params": obj.get("notebook_params"),
                "python_params": obj.get("python_params"),
                "spark_submit_params": obj.get("spark_submit_params"),
                "python_named_params": obj.get("python_named_params"),
                "pipeline_params": RunParametersPipelineParams.from_dict(
                    obj.get("pipeline_params")
                )
                if obj.get("pipeline_params") is not None
                else None,
                "sql_params": obj.get("sql_params"),
                "dbt_commands": obj.get("dbt_commands"),
            }
        )
        return _obj
